NS-HYB-000 — C++/Python Hybrid Stack Concept (Initial)

0. Purpose

Define an initial C++/Python hybrid implementation concept for Northfield Solidarity (NS): Python remains the orchestration and productivity layer; C++ provides a native acceleration lane for performance, determinism, memory control, and later deep-AI / systems kernels.

This document is concept-first: stack choices, boundaries, packaging, CI/CD shape, and diagrams.

1. Core Thesis

Python owns workflows: APIs, event flows, data plumbing, operator tooling, rapid iteration.

C++ owns hotspots: compute kernels, graph/index primitives, simulation loops, parsers, compression, memory-sensitive structures.

The boundary is explicit: stable interfaces, versioned contracts, and measurable SLAs.

Default rule: Write it in Python. Profile. Then promote hotspots to C++.

2. Scope

In-scope

A two-lane architecture: Python lane + Native lane (C++).

Packaging + distribution approach for native components.

API boundary patterns: in-process (FFI) and out-of-process (service).

Observability, performance regression gates, and safety checks.

Out-of-scope (for now)

A full engine-by-engine refactor.

Detailed code implementation.

GPU/CUDA build details (kept as extension path).

3. Architectural Options

Option A — In-process Native Extensions (Default)

Python imports C++ as a compiled extension module.

Pros: lowest latency, simplest call path, great for kernels.

Cons: memory safety + ABI compatibility + wheel builds.

Option B — Native Microservice (Isolation)

C++ runs as a separate process with RPC.

Pros: isolation, independent scaling, easier language-agnostic usage.

Cons: RPC overhead, deployment + orchestration cost.

Recommended policy

Start with Option A for pure compute kernels.

Use Option B for stateful native systems (custom stores, long-running simulation workers, memory-hungry processes).

4. Boundary Contracts

4.1 Data Contract Principles

Prefer columnar / contiguous formats for throughput.

Avoid Python object graphs crossing the boundary.

Make ownership rules explicit: who allocates, who frees, who copies.

4.2 Recommended interchange formats

In-process: NumPy arrays / buffer protocol, Arrow arrays, memoryviews.

Service: gRPC with protobuf for control; Arrow Flight / IPC for bulk data.

4.3 API Versioning

Every native module exposes:

module_version (semantic version)

abi_version (integer)

capabilities (feature flags)

Python layer enforces compatibility at import/handshake.

5. The Initial Stack

5.1 Python Lane

Runtime: Python 3.12+ (align with your platform)

Package mgmt: Poetry/uv (choose one standard)

Data: NumPy, PyArrow

Perf: profiling (py-spy), vectorization first

Service APIs: FastAPI (or equivalent) where needed

5.2 C++ Native Lane

Build: CMake

Dependencies: vcpkg or Conan (pick one for consistency)

Toolchain: clang/gcc; sanitizers in CI (ASan/UBSan)

Formatting/lint: clang-format + clang-tidy

Tests: Catch2 or GoogleTest

Perf: perf, heaptrack; benchmark harness

5.3 Python↔C++ Binding

Default: pybind11

Wheel builds: manylinux (x86_64) + macOS arm64; windows if required later

Release artifacts: versioned wheels published to an internal registry

6. Reference Architecture Diagrams

6.1 Hybrid Stack Overview (Two-Lane)

flowchart LR
  subgraph PY[Python Lane]
    API[API / Orchestration]
    WF[Workflows / Pipelines]
    OBS[Observability SDK]
    DATA[Data Access Layer]
  end

  subgraph NAT[Native Lane (C++)]
    K1[Kernel: Graph/Index]
    K2[Kernel: SIMD/Parsing]
    K3[Kernel: Simulation Loop]
  end

  API --> WF --> DATA
  WF -->|In-process FFI| K1
  WF -->|In-process FFI| K2
  WF -->|In-process FFI| K3

  OBS <--> API
  OBS <--> NAT

6.2 In-Process Extension Path (Option A)

sequenceDiagram
  participant P as Python Workflow
  participant M as Native Module (pybind11)
  participant C as C++ Kernel

  P->>M: call(kernel_fn, buffers, params)
  M->>C: pass pointers + metadata
  C-->>M: return status + results buffer
  M-->>P: return numpy/arrow view

6.3 Out-of-Process Native Service (Option B)

flowchart TB
  P[Python Orchestrator] -->|gRPC control| S[Native Service]
  P -->|Arrow IPC/Flight bulk| S
  S --> K[C++ Kernels]
  S --> MEM[Custom Memory/Store]
  S --> OBS[Metrics/Tracing]

7. Packaging & Distribution Model

7.1 Monorepo layout (concept)

python/ — orchestration packages, services, workflows

native/ — C++ libraries + bindings

native/bindings/python/ — pybind11 modules

ci/ — build + test pipelines

7.2 Artifact strategy

Build C++ as:

pure static/shared library for reuse, and

python wheel for direct use.

Wheels are the primary deployment unit for Option A.

For Option B, build a container image with pinned libc + deps.

8. Observability & Safety Gates

8.1 Required instrumentation

Metrics: call counts, latency histograms, bytes processed

Tracing: span per kernel call (tags: version, input size)

Logs: only at boundary errors (no per-call noise)

8.2 CI quality gates

Unit tests (Python + C++)

Sanitizers (nightly or per-PR for native)

Benchmarks: detect >X% regression on kernel microbenches

ABI compatibility check (where applicable)

9. Promotion Path: Python → C++

A component becomes a native candidate when:

It’s top-3 CPU consumer after profiling

It has stable semantics and test coverage

It benefits from contiguous-memory, vectorized processing

Promotion checklist:

Define kernel interface + buffers

Write property tests in Python

Implement C++ kernel

Bind via pybind11

Add benchmark + regression guard

10. Initial “Kernel Catalog” (Seed)

Start with a small set of native kernels that are broadly reusable:

Graph primitives: adjacency compression, traversal, scoring

High-throughput parsing: JSON/CSV/line decoding, tokenization

Vector ops: similarity, distance, top-k selection

Simulation loop helpers: deterministic tick engine utilities

11. Extension Paths (Later)

GPU: CUDA kernels exposed behind the same boundary contracts

Deep AI: custom attention kernels, quantization, memory arenas

Storage: native index services (Option B)

12. Immediate Next Deliverables

NS-HYB-001 — Boundary Spec (buffer rules, error semantics, versioning)

NS-HYB-002 — Repo & CI Plan (build matrix, wheel builds, testing)

NS-HYB-003 — Kernel 1 Proposal (pick first hotspot and define API)

